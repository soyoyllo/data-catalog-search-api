import asyncio
import json
import logging
import os
import threading
from contextlib import asynccontextmanager, suppress
from typing import List, Dict, Optional, Any
from pathlib import Path

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from langgraph_sdk import get_client
from langgraph_sdk.client import LangGraphClient
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores.utils import DistanceStrategy
from watchfiles import awatch

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Pydantic ëª¨ë¸ ì •ì˜ (API ì…ì¶œë ¥ í˜•ì‹) ---

class QueryRequest(BaseModel):
    query: str

class ColumnDescription(BaseModel):
    column_name: str
    description: str
    data_type: str
    is_primary_key: bool

class TableSearchResult(BaseModel):
    # [ìˆ˜ì •] score -> similarity_scoreë¡œ ì´ë¦„ ë³€ê²½
    similarity_score: float
    table_name: str
    table_description: str
    # [ì¶”ê°€] OpenMetadata URL í•„ë“œ ì¶”ê°€
    openmetadata_url: str
    column_descriptions: List[ColumnDescription]

class SearchResponse(BaseModel):
    status: str
    original_query: str
    # [ì¶”ê°€] í–¥í›„ LLM ì‘ë‹µì„ ìœ„í•œ í•„ë“œ ì¶”ê°€
    llm_response: Optional[str] = None
    results: Optional[List[TableSearchResult]] = None


class ChatRequest(BaseModel):
    message: str = Field(..., description="ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸")
    thread_id: Optional[str] = Field(default=None, description="ì´ì–´ê°ˆ LangGraph ìŠ¤ë ˆë“œ ID")
    assistant_id: Optional[str] = Field(default=None, description="ì‚¬ìš©í•  LangGraph assistant/graph ID")
    metadata: Optional[Dict[str, Any]] = Field(default=None, description="ëŸ° ë©”íƒ€ë°ì´í„°")
    config: Optional[Dict[str, Any]] = Field(default=None, description="assistant ì‹¤í–‰ ì„¤ì •")
    context: Optional[Dict[str, Any]] = Field(default=None, description="ëŸ°íƒ€ì„ ì»¨í…ìŠ¤íŠ¸")


class ChatResponse(BaseModel):
    thread_id: str
    assistant_message: Optional[str] = None
    
class UpdateRequest(BaseModel):
    metadata_path: Optional[str] = Field(
        default="metadata/enriched_metadata_clustered.json",
        description="ì»¨í…Œì´ë„ˆ ê¸°ì¤€ ë©”íƒ€ë°ì´í„° JSON ê²½ë¡œ",
        example="metadata/enriched_metadata_clustered.json"
    )


# --- ê²€ìƒ‰ ì—”ì§„ ë¡œì§ ---

def load_metadata(file_path: Path) -> List[Dict]:
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def create_and_save_faiss_index(metadata_list: List[Dict], index_path: Path, embedding_model: SentenceTransformerEmbeddings):
    """FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    documents, metadatas = [], []
    for table in metadata_list:
        table_name = table['name']
        column_texts = [f"- ì»¬ëŸ¼ '{col['name']}': {col['description']}" for col in table.get('columns', [])]
        full_doc = f"í…Œì´ë¸”ëª…: {table_name}\ní…Œì´ë¸” ì„¤ëª…: {table['description']}\ní¬í•¨ëœ ì»¬ëŸ¼ ì •ë³´:\n" + "\n".join(column_texts)
        documents.append(full_doc)
        metadatas.append({"type": "table", "table_name": table_name})
    
    logger.info(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤...")
    embeddings = embedding_model.embed_documents(documents)
    
    text_embedding_pairs = list(zip(documents, embeddings))
    
    logger.info("ì½”ì‚¬ì¸ ê±°ë¦¬ ë°©ì‹ìœ¼ë¡œ ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    vector_db = FAISS.from_embeddings(
        text_embeddings=text_embedding_pairs, 
        embedding=embedding_model, 
        metadatas=metadatas, 
        distance_strategy=DistanceStrategy.COSINE
    )
    
    vector_db.save_local(str(index_path))
    logger.info(f"âœ… ìƒˆë¡œìš´ ì¸ë±ìŠ¤ë¥¼ '{index_path}' íŒŒì¼ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    return vector_db

# [ìˆ˜ì •] OpenMetadata URLì„ ì¸ìë¡œ ë°›ì•„ ìµœì¢… ê²°ê³¼ë¥¼ ìƒì„±í•˜ë„ë¡ ë³€ê²½
def search_and_format_results(query: str, vector_db: FAISS, all_metadata_dict: Dict, openmetadata_base_url: str, top_k: int = 3) -> Optional[List[Dict]]:
    """ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìƒì„¸í•œ JSON í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤."""
    similar_docs = vector_db.similarity_search_with_score(query, k=top_k)
    
    logger.info(f"ğŸ” ë²¡í„° DB ê²€ìƒ‰ ê²°ê³¼ (Top {top_k}):")
    for i, (doc, score) in enumerate(similar_docs):
        log_content = doc.page_content.replace('\n', ' ')
        logger.info(f"  - [{i+1}] ìœ ì‚¬ë„ ì ìˆ˜(ì½”ì‚¬ì¸ ê±°ë¦¬): {score:.4f}, ë‚´ìš©: \"{log_content[:80]}...\"")

    if not similar_docs: return None
    
    found_tables = {}
    threshold = 0.3  # 70% ì´ìƒ ê´€ë ¨ë„ (1 - 0.3 = 0.7 = 70%)  
    
    # ì •í™•í•œ í…Œì´ë¸”ëª… ë§¤ì¹­ ìš°ì„ ìˆœìœ„ ì²˜ë¦¬
    query_upper = query.upper()
    exact_match_found = False
    
    # ë¨¼ì € ì •í™•í•œ í…Œì´ë¸”ëª… ë§¤ì¹­ í™•ì¸
    for doc, score in similar_docs:
        table_name = doc.metadata['table_name']
        table_name_upper = table_name.upper()
        
        # ì •í™•í•œ í…Œì´ë¸”ëª… ë§¤ì¹­ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
        if query_upper == table_name_upper:
            found_tables[table_name] = 0.0  # ì •í™•í•œ ë§¤ì¹­ì€ ìµœê³  ì ìˆ˜
            exact_match_found = True
            logger.info(f"    -> ğŸ¯ ì •í™•í•œ í…Œì´ë¸”ëª… ë§¤ì¹­: '{table_name}' (ì ìˆ˜: 0.0)")
            break
    
    # ëª¨ë“  ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ (ì •í™•í•œ ë§¤ì¹­ì´ ìˆì–´ë„ ë‚˜ë¨¸ì§€ ìœ ì‚¬í•œ ê²°ê³¼ í¬í•¨)
    for doc, score in similar_docs:
        table_name = doc.metadata['table_name']
        table_name_upper = table_name.upper()
        
        # ì •í™•í•œ ë§¤ì¹­ì€ ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
        if exact_match_found and query_upper == table_name_upper:
            continue
            
        # ì„ê³„ê°’ í†µê³¼í•˜ëŠ” í…Œì´ë¸”ë“¤ ì¶”ê°€
        if score < threshold:
            if table_name not in found_tables or score < found_tables[table_name]:
                found_tables[table_name] = score
                logger.info(f"    -> âœ… ì„ê³„ê°’ í†µê³¼. í…Œì´ë¸” '{table_name}' ì¶”ê°€/ê°±ì‹  (ì ìˆ˜: {score:.4f}).")

    if not found_tables and similar_docs:
        top_doc, top_score = similar_docs[0]
        table_name = top_doc.metadata['table_name']
        logger.info(f"    -> âŒ ì„ê³„ê°’ í†µê³¼ ê²°ê³¼ ì—†ìŒ. ê°€ì¥ ìœ ì‚¬í•œ '{table_name}'ì˜ ì ìˆ˜: {top_score:.4f} (ì„ê³„ê°’: {threshold})")
        logger.info(f"    -> ê´€ë ¨ë„ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None  # ì„ê³„ê°’ì„ ë„˜ëŠ” ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜

    results_list = []
    for table_name, score in sorted(found_tables.items(), key=lambda item: item[1]):
        table_data = all_metadata_dict.get(table_name)
        if table_data:
            results_list.append({
                "similarity_score": round(score, 4),
                "table_name": table_name,
                "table_description": table_data['description'],
                "openmetadata_url": f"{openmetadata_base_url}/explore/?search={table_name}&sort=_score&page=1&size=15",
                "column_descriptions": [
                    {
                        "column_name": col['name'], 
                        "description": col['description'],
                        "data_type": col.get('dataTypeDisplay', 'N/A'),
                        "is_primary_key": col.get('isPrimaryKey', False)
                    } 
                    for col in table_data.get('columns', [])
                ]
            })
    return results_list

# --- FastAPI ì•± ìƒëª…ì£¼ê¸° ë° ì „ì—­ ë³€ìˆ˜ ì„¤ì • ---
search_engine_globals = {}
_index_refresh_lock = threading.Lock()

DEFAULT_OPENMETADATA_URL = "http://localhost:8585/"
CONFIG_FILE_ENV_VAR = "OPENMETADATA_CONFIG_FILE"
METADATA_FILE_ENV_VAR = "METADATA_FILE_PATH"
FAISS_INDEX_DIR_ENV_VAR = "FAISS_INDEX_DIR"

LANGGRAPH_API_URL = "LANGGRAPH_API_URL"
LANGGRAPH_ASSISTANT_ID = "LANGGRAPH_ASSISTANT_ID"
OPENAI_API_KEY = "OPENAI_API_KEY"


async def get_langgraph_client(api_key: Optional[str] = None) -> LangGraphClient:
    """Create a LangGraph client using the provided API key (or fallback env)."""

    base_url = os.getenv(LANGGRAPH_API_URL)
    if not base_url:
        raise HTTPException(status_code=503, detail="LangGraph API URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    resolved_key = api_key if api_key is not None else os.getenv(OPENAI_API_KEY)

    try:
        return get_client(url=base_url, api_key=resolved_key)
    except Exception as exc:  # pragma: no cover - defensive logging
        logger.error("LangGraph í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: %s", exc, exc_info=True)
        raise HTTPException(status_code=503, detail="LangGraph í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") from exc


async def ensure_thread(client: LangGraphClient, thread_id: Optional[str]) -> str:
    """Return an existing thread id or create a fresh thread."""

    if thread_id:
        return thread_id

    thread = await client.threads.create(metadata={"source": "data-catalog-search-api"})
    return thread["thread_id"]


def extract_assistant_message(state: Dict[str, Any]) -> Optional[str]:
    """Pick the latest assistant message from the state values."""

    values = state.get("values")
    messages: Optional[List[Any]] = None

    if isinstance(values, dict):
        maybe_messages = values.get("messages")
        if isinstance(maybe_messages, list):
            messages = maybe_messages

    if not messages:
        return None

    for message in reversed(messages):
        if not isinstance(message, dict):
            continue
        role = message.get("type") or message.get("role")
        if role not in {"ai", "assistant"}:
            continue
        content = message.get("content")
        if isinstance(content, str):
            return content
        if isinstance(content, list):
            segments = []
            for chunk in content:
                if isinstance(chunk, dict):
                    text = chunk.get("text")
                    if text:
                        segments.append(text)
            if segments:
                return "".join(segments)
        if isinstance(content, dict):
            text = content.get("text")
            if isinstance(text, str):
                return text

    return None


def _read_openmetadata_url_from_file(config_path: Path) -> Optional[str]:
    """Parse a simple KEY=VALUE config file and extract OPENMETADATA_BASE_URL."""
    if not config_path.exists():
        return None

    try:
        with open(config_path, "r", encoding="utf-8") as cfg:
            for raw_line in cfg:
                line = raw_line.strip()
                if not line or line.startswith("#"):
                    continue
                if line.startswith("OPENMETADATA_BASE_URL="):
                    return line.split("=", 1)[1].strip()
                # ë‹¨ì¼ ê°’ë§Œ ìˆëŠ” ê²½ìš°ë„ í—ˆìš© (ì˜ˆ: URLë§Œ ì ì–´ë‘” íŒŒì¼)
                if "=" not in line:
                    return line
    except Exception as exc:
        logger.warning("ì„¤ì • íŒŒì¼ '%s'ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜: %s", config_path, exc)
    return None


def load_openmetadata_base_url() -> str:
    """Resolve the current OpenMetadata base URL from environment or config file."""
    # 1) í™˜ê²½ ë³€ìˆ˜ ìš°ì„ 
    env_override = os.getenv("OPENMETADATA_BASE_URL")
    if env_override:
        return env_override

    # 2) ì„¤ì • íŒŒì¼ì—ì„œ ì½ê¸°
    config_file = Path(os.getenv(CONFIG_FILE_ENV_VAR, ".env"))
    file_value = _read_openmetadata_url_from_file(config_file)
    if file_value:
        return file_value

    # 3) ê¸°ë³¸ê°’ ë°˜í™˜
    return DEFAULT_OPENMETADATA_URL


def derive_faiss_index_path(metadata_path: Path) -> Path:
    base_dir = Path(os.getenv(FAISS_INDEX_DIR_ENV_VAR, "faiss_indices"))
    safe_stem = metadata_path.stem.replace(" ", "_").replace(".", "_")
    return base_dir / safe_stem


def refresh_faiss_index_if_needed(metadata_path: Optional[Path] = None) -> Dict[str, str]:
    """Rebuild the FAISS index if the metadata JSON changed.

    Returns a dict with keys:
        status: "updated" or "skipped"
        detail: human-readable explanation
    Raises HTTPException on hard failures (missing config, JSON error, etc.).
    """
    metadata_path = Path(metadata_path) if metadata_path else search_engine_globals.get('metadata_path')
    embedding_model: Optional[SentenceTransformerEmbeddings] = search_engine_globals.get('embedding_model')

    if metadata_path is None or embedding_model is None:
        logger.error("ë©”íƒ€ë°ì´í„° ê²½ë¡œ ë˜ëŠ” ì¸ë±ìŠ¤ ì„¤ì •ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        raise HTTPException(status_code=503, detail="Metadata or index configuration is missing.")

    metadata_path = metadata_path if isinstance(metadata_path, Path) else Path(metadata_path)
    metadata_mtime_map: Dict[str, float] = search_engine_globals.setdefault('metadata_mtime_map', {})
    metadata_key = str(metadata_path.resolve())

    path_changed = search_engine_globals.get('metadata_path') != metadata_path
    if path_changed:
        search_engine_globals['metadata_path'] = metadata_path
        search_engine_globals['faiss_index_path'] = derive_faiss_index_path(metadata_path)
        search_engine_globals['metadata_mtime'] = metadata_mtime_map.get(metadata_key)
        search_engine_globals.pop('vector_db', None)

    faiss_index_path: Optional[Path] = search_engine_globals.get('faiss_index_path')
    if faiss_index_path is None:
        faiss_index_path = derive_faiss_index_path(metadata_path)
        search_engine_globals['faiss_index_path'] = faiss_index_path

    try:
        current_mtime = metadata_path.stat().st_mtime
    except FileNotFoundError:
        msg = f"ë©”íƒ€ë°ì´í„° íŒŒì¼ '{metadata_path}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        logger.error(msg)
        raise HTTPException(status_code=404, detail=msg)

    last_mtime = search_engine_globals.get('metadata_mtime')
    index_exists = faiss_index_path.exists()
    if not path_changed and index_exists and last_mtime and current_mtime <= last_mtime:
        return {"status": "skipped", "detail": "Metadata unchanged."}

    with _index_refresh_lock:
        last_mtime = search_engine_globals.get('metadata_mtime')
        index_exists = faiss_index_path.exists()
        if path_changed and index_exists:
            cached_mtime = metadata_mtime_map.get(metadata_key)
            if cached_mtime and current_mtime <= cached_mtime:
                logger.info("ğŸ“‚ ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: %s", faiss_index_path)
                metadata = load_metadata(metadata_path)
                search_engine_globals['metadata_dict'] = {table['name']: table for table in metadata}

                search_engine_globals['vector_db'] = FAISS.load_local(
                    str(faiss_index_path),
                    embedding_model,
                    allow_dangerous_deserialization=True,
                    distance_strategy=DistanceStrategy.COSINE
                )
                search_engine_globals['metadata_mtime'] = cached_mtime
                return {"status": "loaded", "detail": "Existing FAISS index loaded for metadata path."}

        if not path_changed and index_exists and last_mtime and current_mtime <= last_mtime:
            return {"status": "skipped", "detail": "Metadata unchanged."}

        logger.info("ğŸ“ ë©”íƒ€ë°ì´í„° íŒŒì¼ ë³€ê²½ ê°ì§€. FAISS ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í•©ë‹ˆë‹¤...")
        try:
            metadata = load_metadata(metadata_path)
        except json.JSONDecodeError as exc:
            msg = f"ë©”íƒ€ë°ì´í„° JSON íŒŒì‹± ì‹¤íŒ¨: {exc}"
            logger.error(msg)
            raise HTTPException(status_code=400, detail=msg)

        search_engine_globals['metadata_dict'] = {table['name']: table for table in metadata}

        faiss_index_path.parent.mkdir(parents=True, exist_ok=True)
        search_engine_globals['vector_db'] = create_and_save_faiss_index(metadata, faiss_index_path, embedding_model)
        search_engine_globals['metadata_mtime'] = current_mtime
        metadata_mtime_map[metadata_key] = current_mtime
        logger.info("âœ… ë©”íƒ€ë°ì´í„° ë° ì¸ë±ìŠ¤ê°€ ìµœì‹  ìƒíƒœë¡œ ê°±ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return {"status": "updated", "detail": "Metadata and FAISS index refreshed."}


async def monitor_openmetadata_base_url():
    """Watch the config file for changes and refresh the OpenMetadata URL in realtime."""
    config_file = Path(os.getenv(CONFIG_FILE_ENV_VAR, ".env")).resolve()
    config_file.parent.mkdir(parents=True, exist_ok=True)

    watch_target = config_file if config_file.exists() else config_file.parent
    last_value = search_engine_globals.get("openmetadata_base_url")

    logger.info("ğŸ” '%s' ê°ì‹œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.", watch_target)

    async for changes in awatch(watch_target):
        relevant_change = False
        config_path_resolved = config_file.resolve()
        for _change, changed_path in changes:
            if Path(changed_path).resolve() == config_path_resolved:
                relevant_change = True
                break

        if not relevant_change and watch_target == config_file:
            # watch_targetì´ íŒŒì¼ì¸ë° ë‹¤ë¥¸ ë³€ê²½ì´ë©´ ë¬´ì‹œ
            continue

        current_value = _read_openmetadata_url_from_file(config_file) or DEFAULT_OPENMETADATA_URL
        if current_value != last_value:
            search_engine_globals["openmetadata_base_url"] = current_value
            logger.info("ğŸ” OpenMetadata URL ì—…ë°ì´íŠ¸: %s", current_value)
            last_value = current_value
        else:
            logger.info("ğŸ” OpenMetadata ì„¤ì • íŒŒì¼ì´ ê°±ì‹ ë˜ì—ˆì§€ë§Œ URLì€ ë³€ê²½ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("ğŸš€ ì„œë²„ ì‹œì‘: ê²€ìƒ‰ ì—”ì§„ì„ ì„¤ì •í•©ë‹ˆë‹¤...")
    
    # OpenMetadata URL ì´ˆê¸°í™” ë° ë™ì  ê°±ì‹  íƒœìŠ¤í¬ ì‹œì‘
    initial_url = load_openmetadata_base_url()
    search_engine_globals['openmetadata_base_url'] = initial_url
    logger.info(f"OpenMetadata ê¸°ë³¸ URL: {initial_url}")

    METADATA_FILE_PATH = Path(os.getenv(METADATA_FILE_ENV_VAR, 'metadata/enriched_metadata_clustered.json'))
    FAISS_INDEX_PATH = derive_faiss_index_path(METADATA_FILE_PATH)

    search_engine_globals['metadata_path'] = METADATA_FILE_PATH
    search_engine_globals['faiss_index_path'] = FAISS_INDEX_PATH

    all_metadata = load_metadata(METADATA_FILE_PATH)
    search_engine_globals['metadata_dict'] = {table['name']: table for table in all_metadata}
    
    logger.info("E5-Small ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
    embedding_model = SentenceTransformerEmbeddings(
        model_name="intfloat/multilingual-e5-small",
        model_kwargs={"device": "cpu"}
    )
    search_engine_globals['embedding_model'] = embedding_model

    vector_db = None
    if not FAISS_INDEX_PATH.exists():
        FAISS_INDEX_PATH.parent.mkdir(parents=True, exist_ok=True)
        logger.warning(f"ì¸ë±ìŠ¤ íŒŒì¼ '{FAISS_INDEX_PATH}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        vector_db = create_and_save_faiss_index(all_metadata, FAISS_INDEX_PATH, embedding_model)
    
    if vector_db is None:
        logger.info(f"'{FAISS_INDEX_PATH}'ì—ì„œ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        vector_db = FAISS.load_local(
            str(FAISS_INDEX_PATH), 
            embedding_model, 
            allow_dangerous_deserialization=True,
            distance_strategy=DistanceStrategy.COSINE 
        )

    search_engine_globals['vector_db'] = vector_db

    try:
        search_engine_globals['metadata_mtime'] = METADATA_FILE_PATH.stat().st_mtime
    except FileNotFoundError:
        search_engine_globals['metadata_mtime'] = None
    
    if search_engine_globals.get('metadata_mtime') is not None:
        metadata_mtime_map = search_engine_globals.setdefault('metadata_mtime_map', {})
        metadata_mtime_map[str(METADATA_FILE_PATH.resolve())] = search_engine_globals['metadata_mtime']

    logger.info("âœ… ê²€ìƒ‰ ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ.")

    config_task = None
    if os.getenv("OPENMETADATA_BASE_URL"):
        logger.info("í™˜ê²½ ë³€ìˆ˜ OPENMETADATA_BASE_URLì´ ì„¤ì •ë˜ì–´ ìˆì–´ íŒŒì¼ ë³€ê²½ ê°ì‹œë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
    else:
        config_task = asyncio.create_task(monitor_openmetadata_base_url())
    try:
        yield
    finally:
        if config_task:
            config_task.cancel()
            with suppress(asyncio.CancelledError):
                await config_task
        search_engine_globals.clear()
        logger.info("ğŸ‘‹ ì„œë²„ ì¢…ë£Œ.")

# --- FastAPI ì•± ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ---
app = FastAPI(
    title="ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ API (E5-small)",
    description="ìì—°ì–´ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í…Œì´ë¸” ë° ì»¬ëŸ¼ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
    version="2.8", # ë²„ì „ ì—…ë°ì´íŠ¸
    lifespan=lifespan
)

# --- CORS ì„¤ì • ì¶”ê°€ ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ë„ë©”ì¸ì—ì„œ ì ‘ê·¼ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],  # ëª¨ë“  HTTP ë©”ì„œë“œ í—ˆìš©
    allow_headers=["*"],  # ëª¨ë“  í—¤ë” í—ˆìš©
)

# --- API ì—”ë“œí¬ì¸íŠ¸ ---
@app.post("/search", response_model=SearchResponse)
async def search_metadata(request: QueryRequest):
    query = request.query
    logger.info(f"ğŸ“¬ ìˆ˜ì‹ ëœ ì¿¼ë¦¬: {query}")
    vector_db = search_engine_globals.get('vector_db')
    metadata_dict = search_engine_globals.get('metadata_dict')
    openmetadata_base_url = search_engine_globals.get('openmetadata_base_url')

    if not all([vector_db, metadata_dict, openmetadata_base_url]):
        raise HTTPException(status_code=503, detail="ê²€ìƒ‰ ì—”ì§„ì´ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    try:
        search_results = search_and_format_results(query, vector_db, metadata_dict, openmetadata_base_url)
        return SearchResponse(
            status="success", 
            original_query=query, 
            # [ì¶”ê°€] llm_response í•„ë“œì— ê¸°ë³¸ê°’ ì¶”ê°€
            llm_response="LLM ì‘ë‹µì€ í–¥í›„ ì¶”ê°€ë  ì˜ˆì •ì…ë‹ˆë‹¤.",
            results=search_results
        )
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


@app.post("/chat", response_model=ChatResponse)
async def chat_with_langgraph(payload: ChatRequest, request: Request) -> ChatResponse:
    """Proxy chat requests to a LangGraph assistant."""
    
    assistant_id = payload.assistant_id or os.getenv(LANGGRAPH_ASSISTANT_ID)
    if not assistant_id:
        raise HTTPException(status_code=500, detail="LangGraph assistant IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    api_key = request.headers.get("x-api-key")
    logger.info(
        "ğŸ’¬ /chat ì‹œì‘: assistant_id=%s, thread=%s, has_api_key=%s",
        assistant_id,
        payload.thread_id,
        bool(api_key),
    )
    client = await get_langgraph_client(api_key=api_key)

    try:
        thread_id = await ensure_thread(client, payload.thread_id)
        logger.info("ğŸ§µ thread ì¤€ë¹„ ì™„ë£Œ: %s", thread_id)

        config_payload: Optional[Dict[str, Any]] = payload.config.copy() if payload.config else {}
        if config_payload is not None:
            configurable = dict(config_payload.get("configurable", {}))
        else:
            configurable = {}
        if api_key:
            configurable.setdefault("openai_api_key", api_key)
        if assistant_id:
            configurable.setdefault("assistant_id", assistant_id)
        if configurable:
            if config_payload is None:
                config_payload = {}
            config_payload["configurable"] = configurable
        elif config_payload is not None and "configurable" in config_payload:
            config_payload.pop("configurable")

        if config_payload == {}:
            config_payload = None

        logger.info(
            "ğŸš€ runs.wait í˜¸ì¶œ: thread=%s, assistant=%s, metadata=%s, config=%s",
            thread_id,
            assistant_id,
            payload.metadata,
            config_payload,
        )
        await client.runs.wait(
            thread_id=thread_id,
            assistant_id=assistant_id,
            input={"messages": [{"role": "user", "content": payload.message}]},
            metadata=payload.metadata,
            config=config_payload,
            context=payload.context,
        )

        state = await client.threads.get_state(thread_id=thread_id)
        logger.debug("ğŸ“¥ threads.get_state ì™„ë£Œ: %s", state)
    except HTTPException:
        raise
    except Exception as exc:  # pragma: no cover - defensive logging
        logger.error("LangGraph ì±— ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: %s", exc, exc_info=True)
        raise HTTPException(status_code=500, detail="LangGraph ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.") from exc
    finally:
        with suppress(Exception):
            await client.aclose()
        logger.debug("ğŸ”š LangGraph í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ")

    raw_metadata = state.get("metadata")
    metadata = raw_metadata if isinstance(raw_metadata, dict) else {}
    assistant_message = extract_assistant_message(state)
    logger.debug("ğŸ’¡ assistant_message=%s", assistant_message)

    return ChatResponse(
        thread_id=thread_id,
        assistant_message=assistant_message,
    )


@app.post("/update")
async def trigger_metadata_refresh(body: UpdateRequest = UpdateRequest()):
    logger.info("ğŸ“¦ /update ìš”ì²­ ìˆ˜ì‹ : ë©”íƒ€ë°ì´í„° ë° ì¸ë±ìŠ¤ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.")

    metadata_path = Path(body.metadata_path) if body.metadata_path else None
    result = refresh_faiss_index_if_needed(metadata_path)

    if 'vector_db' not in search_engine_globals:
        logger.error("FAISS ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        raise HTTPException(status_code=503, detail="ê²€ìƒ‰ ì—”ì§„ì´ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return result
